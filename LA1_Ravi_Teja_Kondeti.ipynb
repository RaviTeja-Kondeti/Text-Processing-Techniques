{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Title: Text Analysis of Restaurant Reviews\n",
        "# Author: Ravi Teja Kondeti\n",
        "# ASU ID: 1234434879\n",
        "# File Creation Date: January 25, 2025\n"
      ],
      "metadata": {
        "id": "gNLnVOLT4Lfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Library and Data Import\n",
        "import pandas as pd              # For data manipulation and analysis\n",
        "import spacy                     # For text processing (NLP)\n",
        "from collections import Counter  # For counting word frequencies\n",
        "\n",
        "# Load the dataset with the first 1000 rows only\n",
        "data = pd.read_csv('restaurant_reviews_az.csv', nrows=1000)\n",
        "\n",
        "# Summary of the input data\n",
        "print(data.info())\n",
        "print(data.describe(include='all'))\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMgRgyzy3kBe",
        "outputId": "95268843-21d3-49df-b460-381ac73d1558"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 10 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   review_id    1000 non-null   object\n",
            " 1   user_id      1000 non-null   object\n",
            " 2   business_id  1000 non-null   object\n",
            " 3   stars        1000 non-null   int64 \n",
            " 4   useful       1000 non-null   int64 \n",
            " 5   funny        1000 non-null   int64 \n",
            " 6   cool         1000 non-null   int64 \n",
            " 7   text         1000 non-null   object\n",
            " 8   date         1000 non-null   object\n",
            " 9   Sentiment    1000 non-null   int64 \n",
            "dtypes: int64(5), object(5)\n",
            "memory usage: 78.3+ KB\n",
            "None\n",
            "       review_id user_id             business_id        stars       useful  \\\n",
            "count       1000    1000                    1000  1000.000000  1000.000000   \n",
            "unique       981     897                     146          NaN          NaN   \n",
            "top       #NAME?  #NAME?  UCMSWPqzXjd7QHq7v8PJjQ          NaN          NaN   \n",
            "freq          20      10                      74          NaN          NaN   \n",
            "mean         NaN     NaN                     NaN     3.966000     0.700000   \n",
            "std          NaN     NaN                     NaN     1.439763     1.650674   \n",
            "min          NaN     NaN                     NaN     1.000000     0.000000   \n",
            "25%          NaN     NaN                     NaN     3.000000     0.000000   \n",
            "50%          NaN     NaN                     NaN     5.000000     0.000000   \n",
            "75%          NaN     NaN                     NaN     5.000000     1.000000   \n",
            "max          NaN     NaN                     NaN     5.000000    29.000000   \n",
            "\n",
            "              funny         cool  \\\n",
            "count   1000.000000  1000.000000   \n",
            "unique          NaN          NaN   \n",
            "top             NaN          NaN   \n",
            "freq            NaN          NaN   \n",
            "mean       0.142000     0.382000   \n",
            "std        0.578075     1.349777   \n",
            "min        0.000000     0.000000   \n",
            "25%        0.000000     0.000000   \n",
            "50%        0.000000     0.000000   \n",
            "75%        0.000000     0.000000   \n",
            "max       10.000000    28.000000   \n",
            "\n",
            "                                                     text             date  \\\n",
            "count                                                1000             1000   \n",
            "unique                                               1000             1000   \n",
            "top     OK, the hype about having Hatch chili in your ...  1/27/2020 22:59   \n",
            "freq                                                    1                1   \n",
            "mean                                                  NaN              NaN   \n",
            "std                                                   NaN              NaN   \n",
            "min                                                   NaN              NaN   \n",
            "25%                                                   NaN              NaN   \n",
            "50%                                                   NaN              NaN   \n",
            "75%                                                   NaN              NaN   \n",
            "max                                                   NaN              NaN   \n",
            "\n",
            "          Sentiment  \n",
            "count   1000.000000  \n",
            "unique          NaN  \n",
            "top             NaN  \n",
            "freq            NaN  \n",
            "mean       0.762000  \n",
            "std        0.426072  \n",
            "min        0.000000  \n",
            "25%        1.000000  \n",
            "50%        1.000000  \n",
            "75%        1.000000  \n",
            "max        1.000000  \n",
            "                review_id                 user_id             business_id  \\\n",
            "0  IVS7do_HBzroiCiymNdxDg  fdFgZQQYQJeEAshH4lxSfQ  sGy67CpJctjeCWClWqonjA   \n",
            "1  QP2pSzSqpJTMWOCuUuyXkQ  JBLWSXBTKFvJYYiM-FnCOQ  3w7NRntdQ9h0KwDsksIt5Q   \n",
            "2  oK0cGYStgDOusZKz9B1qug  2_9fKnXChUjC5xArfF8BLg  OMnPtRGmbY8qH_wIILfYKA   \n",
            "3  E_ABvFCNVLbfOgRg3Pv1KQ  9MExTQ76GSKhxSWnTS901g  V9XlikTxq0My4gE8LULsjw   \n",
            "4  Rd222CrrnXkXukR2iWj69g  LPxuausjvDN88uPr-Q4cQA  CA5BOxKRDPGJgdUQ8OUOpw   \n",
            "\n",
            "   stars  useful  funny  cool  \\\n",
            "0      3       1      1     0   \n",
            "1      5       1      1     1   \n",
            "2      5       1      0     0   \n",
            "3      5       0      0     0   \n",
            "4      4       1      0     0   \n",
            "\n",
            "                                                text             date  \\\n",
            "0  OK, the hype about having Hatch chili in your ...  1/27/2020 22:59   \n",
            "1  Pandemic pit stop to have an ice cream.... onl...   4/19/2020 5:33   \n",
            "2  I was lucky enough to go to the soft opening a...  2/29/2020 19:43   \n",
            "3  I've gone to claim Jumpers all over the US and...  3/14/2020 21:47   \n",
            "4  If you haven't been  to Maynard's kitchen, it'...  1/17/2020 20:32   \n",
            "\n",
            "   Sentiment  \n",
            "0          1  \n",
            "1          1  \n",
            "2          1  \n",
            "3          1  \n",
            "4          1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the 1-star and 5-star reviews\n",
        "# Filter reviews with 1 and 5 stars\n",
        "one_star_reviews = data[data['stars'] == 1]\n",
        "five_star_reviews = data[data['stars'] == 5]\n",
        "\n",
        "# Print the count of 1-star and 5-star reviews to verify selection\n",
        "print(f\"Number of 1-star reviews: {len(one_star_reviews)}\")\n",
        "print(f\"Number of 5-star reviews: {len(five_star_reviews)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSpkrt3h35gi",
        "outputId": "fdbfaa0b-eb08-43a8-aca3-4eefd8201d46"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of 1-star reviews: 132\n",
            "Number of 5-star reviews: 562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply necessary text processing techniques\n",
        "# Load the Spacy English model for natural language processing\n",
        "# This model provides features like tokenization, lemmatization, and POS tagging\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def process_text(text):\n",
        "    doc = nlp(text)\n",
        "    return {\n",
        "        'tokens': [token.text for token in doc],\n",
        "        'lemmas': [token.lemma_ for token in doc],\n",
        "        'pos': [token.pos_ for token in doc],\n",
        "        'entities': [(ent.text, ent.label_) for ent in doc.ents],\n",
        "        'dependencies': [token.dep_ for token in doc]\n",
        "    }\n",
        "\n",
        "# Process reviews\n",
        "one_star_processed = one_star_reviews['text'].apply(process_text)\n",
        "five_star_processed = five_star_reviews['text'].apply(process_text)\n",
        "\n",
        "print(\"Sample processed text:\")\n",
        "print(one_star_processed.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUJEayap4EV5",
        "outputId": "df140d97-1e7e-47b0-f34e-3bbd4369826d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample processed text:\n",
            "5     {'tokens': ['I', 'stay', 'at', 'the', 'Main', ...\n",
            "13    {'tokens': ['I', 'do', 'n't', 'know', 'what', ...\n",
            "18    {'tokens': ['Very', 'bad', 'service', 'call', ...\n",
            "20    {'tokens': ['This', 'place', 'is', 'not', 'wor...\n",
            "22    {'tokens': ['I', 'was', 'so', 'looking', 'forw...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 20 frequently used nouns\n",
        "# Define a function to extract and count POS (e.g., nouns, adjectives)\n",
        "def get_top_nouns(processed_reviews):\n",
        "    nouns = Counter([token for review in processed_reviews for token, pos in zip(review['tokens'], review['pos']) if pos == 'NOUN'])\n",
        "    return nouns.most_common(20)\n",
        "\n",
        "print(\"Top 20 nouns in 1-star reviews:\")\n",
        "print(get_top_nouns(one_star_processed))\n",
        "\n",
        "print(\"Top 20 nouns in 5-star reviews:\")\n",
        "print(get_top_nouns(five_star_processed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxsIlX5i4eGh",
        "outputId": "9fd78bb5-ee58-4544-830b-f077e862cc8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 nouns in 1-star reviews:\n",
            "[('food', 82), ('order', 61), ('time', 48), ('service', 46), ('place', 41), ('minutes', 41), ('people', 25), ('restaurant', 22), ('location', 21), ('customer', 20), ('pizza', 17), ('drive', 17), ('chicken', 16), ('one', 15), ('manager', 14), ('wings', 13), ('sauce', 13), ('cheese', 13), ('phone', 13), ('card', 13)]\n",
            "Top 20 nouns in 5-star reviews:\n",
            "[('food', 291), ('place', 214), ('service', 152), ('time', 139), ('restaurant', 79), ('pizza', 79), ('staff', 63), ('chicken', 55), ('order', 52), ('menu', 50), ('breakfast', 42), ('experience', 42), ('side', 42), ('meal', 40), ('flavor', 40), ('town', 39), ('ramen', 38), ('sauce', 38), ('lunch', 37), ('dinner', 36)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 20 frequently used verbs\n",
        "def get_top_verbs(processed_reviews):\n",
        "    verbs = Counter([token for review in processed_reviews for token, pos in zip(review['tokens'], review['pos']) if pos == 'VERB'])\n",
        "    return verbs.most_common(20)\n",
        "\n",
        "print(\"Top 20 verbs in 1-star reviews:\")\n",
        "print(get_top_verbs(one_star_processed))\n",
        "\n",
        "print(\"Top 20 verbs in 5-star reviews:\")\n",
        "print(get_top_verbs(five_star_processed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiVjawBb4s2C",
        "outputId": "54a1b0e3-3ec8-4113-868a-645f4578f08d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 verbs in 1-star reviews:\n",
            "[('get', 48), ('had', 46), ('go', 40), ('said', 35), ('told', 31), ('got', 27), ('ordered', 27), ('have', 26), ('asked', 22), ('called', 20), ('give', 18), ('order', 17), ('know', 16), ('going', 16), ('came', 16), ('left', 15), ('want', 15), ('used', 14), ('wait', 14), ('tried', 14)]\n",
            "Top 20 verbs in 5-star reviews:\n",
            "[('had', 228), ('have', 135), ('go', 95), ('get', 95), ('recommend', 74), ('love', 68), ('got', 68), ('try', 67), ('ordered', 62), ('made', 56), ('come', 50), ('take', 43), ('eat', 41), ('came', 41), ('make', 37), ('wait', 36), ('has', 36), ('coming', 35), ('order', 35), ('going', 30)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 20 named entities\n",
        "def get_top_entities(processed_reviews):\n",
        "    entities = Counter([entity for review in processed_reviews for entity, _ in review['entities']])\n",
        "    return entities.most_common(20)\n",
        "\n",
        "print(\"Top 20 named entities in 1-star reviews:\")\n",
        "print(get_top_entities(one_star_processed))\n",
        "\n",
        "print(\"Top 20 named entities in 5-star reviews:\")\n",
        "print(get_top_entities(five_star_processed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH20N3JN42FG",
        "outputId": "70ce1fd8-a2a7-45a1-807f-4f78f238795a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 named entities in 1-star reviews:\n",
            "[('two', 13), ('Taco Bell', 9), ('2', 9), ('Tucson', 8), ('3', 8), ('first', 7), ('20', 7), ('today', 6), ('French', 5), ('10', 5), ('Mexican', 5), ('Waffle House', 5), ('Saturday', 5), ('McDonalds', 5), ('15 minutes', 5), ('one', 4), ('1', 4), ('zero', 4), ('20 minutes', 4), ('10 minutes', 4)]\n",
            "Top 20 named entities in 5-star reviews:\n",
            "[('Tucson', 115), ('first', 31), ('Mexican', 27), ('one', 26), ('two', 22), ('French', 20), ('5', 17), ('Arizona', 16), ('2', 15), ('Love', 12), ('3', 12), ('First', 12), ('Persian', 12), ('today', 10), ('Saturday', 9), ('Sunday', 8), ('taco', 8), ('half', 8), ('Yelp', 8), ('Chinese', 8)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation\n",
        "1-star reviews often contain negative adjectives and nouns such as \"bad,\" \"service,\" and \"wait,\" focusing on issues like delays or rude staff.\n",
        "5-star reviews frequently include positive adjectives like \"amazing,\" \"delicious,\" and \"friendly,\" reflecting satisfaction with food quality and customer service.\n",
        "Key to a good restaurant experience appears to be high-quality food, attentive service, and a welcoming ambiance.\n"
      ],
      "metadata": {
        "id": "Kt9e8tu36oYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acknowledgments\n",
        "I used ChatGPT as a reference tool for structuring this assignment. No other GenAI tools were used, and no collaboration occurred.\n"
      ],
      "metadata": {
        "id": "X6o0XBor6wk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbconvert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rGxRXYpm60f7",
        "outputId": "54004ede-f133-4ee5-d3cb-3e703ae8ae9b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (7.16.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (3.1.5)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (3.1.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (5.10.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nbconvert) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (2.18.0)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (5.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert) (1.4.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert) (4.3.6)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from nbclient>=0.5.0->nbconvert) (6.1.12)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert) (4.23.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert) (2.6)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.36.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.22.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.3.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat>=5.7->nbconvert) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the current notebook to HTML\n",
        "!jupyter nbconvert --to html \"/content/ LA1_Ravi Teja_Kondeti.ipynb\"\n",
        "\n",
        "# Download the HTML file\n",
        "from google.colab import files\n",
        "files.download(\"/content/ LA1_Ravi Teja_Kondeti.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qF6NcXLt9NSe",
        "outputId": "019438d3-7b5e-40ed-d775-00e739afd2a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] WARNING | pattern '/content/ LA1_Ravi Teja_Kondeti.ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb)\n",
            "        to various other formats.\n",
            "\n",
            "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "=======\n",
            "The options below are convenience aliases to configurable class-options,\n",
            "as listed in the \"Equivalent to\" description-line of the aliases.\n",
            "To see all configurable class-options for some <cmd>, use:\n",
            "    <cmd> --help-all\n",
            "\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "    Equivalent to: [--Application.log_level=10]\n",
            "--show-config\n",
            "    Show the application's configuration (human-readable format)\n",
            "    Equivalent to: [--Application.show_config=True]\n",
            "--show-config-json\n",
            "    Show the application's configuration (json format)\n",
            "    Equivalent to: [--Application.show_config_json=True]\n",
            "--generate-config\n",
            "    generate default config file\n",
            "    Equivalent to: [--JupyterApp.generate_config=True]\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only\n",
            "            relevant when converting to notebook format)\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
            "--clear-output\n",
            "    Clear output of current file and save in place,\n",
            "            overwriting the existing notebook.\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
            "--coalesce-streams\n",
            "    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document.\n",
            "            This mode is ideal for generating code-free reports.\n",
            "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
            "--allow-chromium-download\n",
            "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
            "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
            "--disable-chromium-sandbox\n",
            "    Disable chromium security sandbox when converting to PDF..\n",
            "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
            "--show-input\n",
            "    Shows code input. This flag is only useful for dejavu users.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
            "--embed-images\n",
            "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
            "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
            "--sanitize-html\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
            "--log-level=<Enum>\n",
            "    Set the log level by value or name.\n",
            "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
            "    Default: 30\n",
            "    Equivalent to: [--Application.log_level]\n",
            "--config=<Unicode>\n",
            "    Full path of a config file.\n",
            "    Default: ''\n",
            "    Equivalent to: [--JupyterApp.config_file]\n",
            "--to=<Unicode>\n",
            "    The export format to be used, either one of the built-in formats\n",
            "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n",
            "            or a dotted object name that represents the import path for an\n",
            "            ``Exporter`` class\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.export_format]\n",
            "--template=<Unicode>\n",
            "    Name of the template to use\n",
            "    Default: ''\n",
            "    Equivalent to: [--TemplateExporter.template_name]\n",
            "--template-file=<Unicode>\n",
            "    Name of the template file to use\n",
            "    Default: None\n",
            "    Equivalent to: [--TemplateExporter.template_file]\n",
            "--theme=<Unicode>\n",
            "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
            "    as prebuilt extension for the lab template)\n",
            "    Default: 'light'\n",
            "    Equivalent to: [--HTMLExporter.theme]\n",
            "--sanitize_html=<Bool>\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
            "    should be set to True by nbviewer or similar tools.\n",
            "    Default: False\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
            "--writer=<DottedObjectName>\n",
            "    Writer class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: 'FilesWriter'\n",
            "    Equivalent to: [--NbConvertApp.writer_class]\n",
            "--post=<DottedOrNone>\n",
            "    PostProcessor class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
            "--output=<Unicode>\n",
            "    Overwrite base name use for output files.\n",
            "                Supports pattern replacements '{notebook_name}'.\n",
            "    Default: '{notebook_name}'\n",
            "    Equivalent to: [--NbConvertApp.output_base]\n",
            "--output-dir=<Unicode>\n",
            "    Directory to write output(s) to. Defaults\n",
            "                                  to output to the directory of each notebook. To recover\n",
            "                                  previous default behaviour (outputting to the current\n",
            "                                  working directory) use . as the flag value.\n",
            "    Default: ''\n",
            "    Equivalent to: [--FilesWriter.build_directory]\n",
            "--reveal-prefix=<Unicode>\n",
            "    The URL prefix for reveal.js (version 3.x).\n",
            "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
            "            of reveal.js.\n",
            "            For speaker notes to work, this must be a relative path to a local\n",
            "            copy of reveal.js: e.g., \"reveal.js\".\n",
            "            If a relative path is given, it must be a subdirectory of the\n",
            "            current directory (from which the server is run).\n",
            "            See the usage documentation\n",
            "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
            "            for more details.\n",
            "    Default: ''\n",
            "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
            "--nbformat=<Enum>\n",
            "    The nbformat version to write.\n",
            "            Use this to downgrade notebooks.\n",
            "    Choices: any of [1, 2, 3, 4]\n",
            "    Default: 4\n",
            "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to html\n",
            "\n",
            "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n",
            "\n",
            "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "\n",
            "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
            "            'classic'. You can specify the flavor of the format used.\n",
            "\n",
            "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
            "\n",
            "            You can also pipe the output to stdout, rather than a file\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "\n",
            "            PDF is generated via latex\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "\n",
            "            You can get (and serve) a Reveal.js-powered slideshow\n",
            "\n",
            "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "\n",
            "            Multiple notebooks can be given at the command line in a couple of\n",
            "            different ways:\n",
            "\n",
            "            > jupyter nbconvert notebook*.ipynb\n",
            "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "\n",
            "            or you can specify the notebooks list in a config file, containing::\n",
            "\n",
            "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "\n",
            "            > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "To see all available configurables, use `--help-all`.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: /content/ LA1_Ravi Teja_Kondeti.html",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0f7356ea5d70>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Download the HTML file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/ LA1_Ravi Teja_Kondeti.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: /content/ LA1_Ravi Teja_Kondeti.html"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iCO6VcZ5D2D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}